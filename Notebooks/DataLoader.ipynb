{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conda Envrionment install instructions\n",
    "\n",
    "1. Open anaconda prompt -> cd C:\\Users\\rs659\\Desktop\\AIPI 590\\Group Project 1\\Github Repo\n",
    "2. Create a folder where you want the env files to be --> mkdir venv\n",
    "3. conda create --prefix \"C:\\Users\\rs659\\Desktop\\AIPI 590\\Group Project 1\\Github Repo\\Scene-Recognition\\venv\" python=3.11.6\n",
    "4. conda activate \"C:\\Users\\rs659\\Desktop\\AIPI 590\\Group Project 1\\Github Repo\\Scene-Recognition\\venv\"\n",
    "5. conda install -p \"C:\\Users\\rs659\\Desktop\\AIPI 590\\Group Project 1\\Github Repo\\Scene-Recognition\\venv\" ipykernel --update-deps --force-reinstall\n",
    "6. cd Scene-Recognition\n",
    "7. pip install -r Requirements.txt\n",
    "8. pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import get_data_loader\n",
    "import torchvision\n",
    "\n",
    "train_split = 0.75\n",
    "full_dataset = torchvision.datasets.ImageFolder(root=\"Data/\")\n",
    "train_size = int(train_split * len(full_dataset))\n",
    "test_size = int((len(full_dataset) - train_size)/2)\n",
    "val_size = len(full_dataset) - train_size - test_size\n",
    "\n",
    "train_size, test_size, val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader, val_dataloader = get_data_loader(batch_size=64,data_dir=\"Data/\", shuffle=True)\n",
    "for a in train_dataloader:\n",
    "    print(a[0].shape, a[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_stdlib_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pip install split-folders\n",
    "\n",
    "# import splitfolders\n",
    "# splitfolders.ratio('Data/', output=\"output\", seed=1337, ratio=(.7, 0.15,0.15)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages \n",
    "import os \n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "#import your model here\n",
    "from log import create_logger\n",
    "from dataloader import get_data_loader, get_data_loader_split\n",
    "from models.resnet import resnet18\n",
    "from models.efficientnet import effnet_s\n",
    "from models.VGG import VGG\n",
    "from datetime import datetime\n",
    "\n",
    "# Add your models here\n",
    "models = {'resnet18': resnet18,\n",
    "         'enet_s':effnet_s,\n",
    "         'vgg':VGG\n",
    "         }\n",
    "\n",
    "# RUN DETAILS\n",
    "# run_name = \"jly_0206_enets_lr1e-6_bs=64\"\n",
    "\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "run_name = f\"run_{timestamp}\"\n",
    "\n",
    "print(run_name)  # Output: run_2024-02-08_143154\n",
    "\n",
    "\n",
    "model_base = 'enet_s'\n",
    "num_epochs = 250\n",
    "bs = 64\n",
    "lr = 1e-6\n",
    "random_seed = 42\n",
    "save_chks = range(num_epochs) # iterable of epochs for which to save the model\n",
    "use_split = True\n",
    "device = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "if device == 'mps':\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "# set up run dir \n",
    "run_dir = os.path.join('saved_models', run_name)\n",
    "os.makedirs(run_dir, exist_ok = True)\n",
    "log, logclose = create_logger(log_filename=os.path.join(run_dir, 'train.log'), display = False)\n",
    "log(f'using device: {device}')\n",
    "log(f'saving models to: {run_dir}')\n",
    "log(f'using base model: {model_base}')\n",
    "log(f'using batch size: {bs}')\n",
    "log(f'learning rate: {lr}')\n",
    "log(f'random seed: {random_seed}')\n",
    "\n",
    "# seed randoms and make deterministic\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "# random.seed(random_seed)\n",
    "torch.backends.cudnn.enabled=False\n",
    "torch.backends.cudnn.deterministic=True\n",
    "\n",
    "# dataloader\n",
    "if use_split==True:\n",
    "    train_dataloader, test_dataloader, val_dataloader = get_data_loader_split(data_dir=\"output/\",  batch_size=bs, shuffle=True)\n",
    "else:\n",
    "    train_dataloader, test_dataloader, val_dataloader = get_data_loader(data_dir=\"Data/\",  batch_size=bs, shuffle=True)\n",
    "\n",
    "# define model \n",
    "model = models[model_base]()\n",
    "model.to(device)\n",
    "\n",
    "# define optimizer and criterion\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# training loop\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "test_loss = []\n",
    "train_metrics = []\n",
    "val_metrics = []\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"epoch: {epoch}\")\n",
    "    log(f'epoch {epoch}')\n",
    "    #training\n",
    "    model.train()\n",
    "    batch_loss = []\n",
    "    batch_metric = []\n",
    "    total_imgs = 0\n",
    "    for i, (_data, _target) in tqdm(enumerate(train_dataloader)): \n",
    "        data = _data.to(device)\n",
    "        target = _target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(data)\n",
    "        loss = criterion(pred, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss.append(loss.item())\n",
    "        batch_metric.append(sum(torch.argmax(pred, dim=1)==target).item())\n",
    "        total_imgs += len(target)\n",
    "    train_loss.append(sum(np.array(batch_loss)/len(train_dataloader)))\n",
    "    log(f'\\ttrain loss: {train_loss[-1]}')\n",
    "    train_metrics.append(sum(batch_metric)/total_imgs) #TODO: add metrics\n",
    "    print(f\"Training Metric --- Train Accuracy: {sum(batch_metric)/total_imgs} ---- Train Loss: {sum(np.array(batch_loss)/len(train_dataloader))}\")\n",
    "    del data \n",
    "    del target\n",
    "    del pred\n",
    "    del loss\n",
    "\n",
    "    # validation\n",
    "    total_imgs = 0\n",
    "    batch_metric = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        batch_loss = []\n",
    "        for i, (_data, _target) in tqdm(enumerate(val_dataloader)): \n",
    "            data = _data.to(device)\n",
    "            target = _target.to(device)\n",
    "            pred = model(data)\n",
    "            loss = criterion(pred, target)\n",
    "            batch_loss.append(loss.item())\n",
    "            batch_metric.append(sum(torch.argmax(pred, dim=1)==target).item())\n",
    "            total_imgs += len(target)\n",
    "        val_loss.append(sum(np.array(batch_loss)/len(val_dataloader)))\n",
    "        log(f'\\tval loss: {val_loss[-1]}')\n",
    "        val_metrics.append(sum(batch_metric)/total_imgs) #TODO: add metrics\n",
    "        print(f\"Validation Metric --- Val Accuracy: {sum(batch_metric)/total_imgs} ---- Val Loss: {sum(np.array(batch_loss)/len(val_dataloader))}\")\n",
    "\n",
    "\n",
    "    if epoch in save_chks: \n",
    "        torch.save(model.state_dict(), os.path.join(run_dir, f'{epoch}.chkpt'))\n",
    "\n",
    "    plt.plot(train_loss, label='train')\n",
    "    plt.plot(val_loss, label='val')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(run_dir, 'loss'))\n",
    "    plt.close()\n",
    "    plt.plot(train_metrics, label='train accuracy')\n",
    "    plt.plot(val_metrics, label='val accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(run_dir, 'accu'))\n",
    "    plt.close()\n",
    "    del data \n",
    "    del target\n",
    "    del pred\n",
    "    del loss\n",
    "\n",
    "    if device == 'mps':\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "\n",
    "# testing\n",
    "with torch.no_grad():\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    batch_loss = []\n",
    "    for i, (_data, _target) in tqdm(enumerate(test_dataloader)): \n",
    "        data = _data.to(device)\n",
    "        target = _target.to(device)\n",
    "        pred = model(data)\n",
    "        loss = criterion(pred, target)\n",
    "        batch_loss.append(loss.item())\n",
    "        batch_metric.append(sum(torch.argmax(pred, dim=1)==target).item()/len(target))\n",
    "    test_loss.append(sum(np.array(batch_loss)/len(test_dataloader)))\n",
    "    log(f'\\tval loss: {val_loss[-1]}')\n",
    "    val_metrics.append(np.mean(batch_metric)) #TODO: add metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "import PIL as pil\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #load image\n",
    "# #resize to 224x224\n",
    "# org_image = pil.Image.open('/Users/rishabhshah/Desktop/AIPI 590/Scene-Recognition/output/test/cathedral/gsun_1a8814d41a9e6565b1b947d8b79c4c39.jpg', mode='r')\n",
    "\n",
    "\n",
    "# model.predict('/Users/rishabhshah/Desktop/AIPI 590/Scene-Recognition/saved_models/run_2024-02-10_150948/resnet18_full_model_0.pt')\n",
    "\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess the image\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    # transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.485,  0.456,  0.406], std=[0.229,  0.224,  0.225]),\n",
    "])\n",
    "\n",
    "# Load and preprocess the image\n",
    "org_image = pil.Image.open('/Users/rishabhshah/Desktop/AIPI 590/Scene-Recognition/output/test/cathedral/gsun_1a8814d41a9e6565b1b947d8b79c4c39.jpg')\n",
    "input_tensor = preprocess(org_image)\n",
    "input_batch = input_tensor.unsqueeze(0)  # create a mini-batch as expected by the model\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch.to('mps'))\n",
    "\n",
    "# Get the predicted class\n",
    "_, predicted = torch.max(output,  1)\n",
    "classes = ['Campsite','Candy Store','Canyon','Castle','Cathedral']\n",
    "\n",
    "print(f'Predicted class: {classes[predicted.item()]}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_model_path = os.path.join('/tmp', st.session_state.model_file.name)\n",
    "# with open(temp_model_path, 'wb') as f:\n",
    "#     f.write(st.session_state.model_file.getvalue())\n",
    "# Load the model from the temporary path\n",
    "# model = torch.load(st.session_state.model_path).to('mps')  \n",
    "# model = torch.load('/Users/rishabhshah/Desktop/AIPI 590/Scene-Recognition/models/saved_models/run_2024-02-11_133708/resnet18_full_model_2.pt').to('mps')\n",
    "model = torch.load('..\\models\\saved_models\\SEER\\seer_regnet32_finetuned_in1k_model_final_checkpoint_phase78.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define the transformation\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,  0.456,  0.406], std=[0.229,  0.224,  0.225]),\n",
    "])\n",
    "\n",
    "# Load the model (make sure the model architecture matches the training)\n",
    "# model = torch.load('path_to_your_model.pt')\n",
    "model = torch.load('..\\models\\saved_models\\SEER\\seer_regnet32_finetuned_in1k_model_final_checkpoint_phase78.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['classy_state_dict']['base_model']['model'].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Load the image\n",
    "image = Image.open('C:\\\\Users\\\\rs659\\\\Desktop\\\\AIPI  590\\\\Group Project  1\\\\Github Repo\\\\Scene-Recognition\\\\data\\\\raw\\\\test\\\\cathedral\\\\gsun_1a8814d41a9e6565b1b947d8b79c4c39.jpg')\n",
    "input_tensor = preprocess(image)\n",
    "input_batch = input_tensor.unsqueeze(0)  # Create a mini-batch as expected by the model\n",
    "\n",
    "# Move the input and model to GPU for speed if available\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    model.to('cuda')\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "\n",
    "# Get the predicted class\n",
    "_, predicted = torch.max(output,  1)\n",
    "print(f'Predicted class: {predicted.item()}')\n",
    "\n",
    "# Optionally, map the predicted index to class name if you have a mapping\n",
    "# class_names = ['class1', 'class2', ...]\n",
    "# print(f'Predicted class: {class_names[predicted.item()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python Converter/caffe2pth_convertor.py \\\n",
    " --prototxt=YOUT_PROTOTXT_PATH \\\n",
    " --caffemodel=YOUT_CAFFEMODEL_PATH \\\n",
    " --pthmodel=OUTPUT_PTHMODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rs659\\AppData\\Local\\Temp\\tmpe_8ne46_\\caffe.proto\n",
      "C:\\Users\\rs659\\AppData\\Local\\Temp\\tmpe_8ne46_ C:\\Users\\rs659\\AppData\\Local\\Temp\\tmpe_8ne46_\\caffe.proto\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcaffemodel2pytorch\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m caffemodel2pytorch\u001b[39m.\u001b[39;49mNet(\n\u001b[0;32m      5\u001b[0m \tprototxt \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mdeploy_10.prototxt\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      6\u001b[0m \tweights \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39msnapshot_iter_765280.caffemodel\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      7\u001b[0m \tcaffe_proto \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mhttps://raw.githubusercontent.com/BVLC/caffe/master/src/caffe/proto/caffe.proto\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[0;32m      8\u001b[0m \t\u001b[39m# caffe_proto = \"C:\\\\Users\\\\rs659\\\\Desktop\\\\AIPI 590\\\\Group Project 1\\\\Github Repo\\\\Scene-Recognition\\\\Notebooks\\\\caffe.proto\"   \u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\rs659\\Desktop\\AIPI 590\\Group Project 1\\Github Repo\\Scene-Recognition\\Notebooks\\caffemodel2pytorch.py:70\u001b[0m, in \u001b[0;36mNet.__init__\u001b[1;34m(self, prototxt, *args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m weights \u001b[39m=\u001b[39m weights \u001b[39mor\u001b[39;00m (args \u001b[39m+\u001b[39m (\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m))[\u001b[39m0\u001b[39m]\n\u001b[0;32m     68\u001b[0m phase \u001b[39m=\u001b[39m phase \u001b[39mor\u001b[39;00m (args \u001b[39m+\u001b[39m (\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m))[\u001b[39m1\u001b[39m]\n\u001b[1;32m---> 70\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet_param \u001b[39m=\u001b[39m initialize(caffe_proto)\u001b[39m.\u001b[39mNetParameter()\n\u001b[0;32m     71\u001b[0m google\u001b[39m.\u001b[39mprotobuf\u001b[39m.\u001b[39mtext_format\u001b[39m.\u001b[39mParse(\u001b[39mopen\u001b[39m(prototxt)\u001b[39m.\u001b[39mread(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet_param)\n\u001b[0;32m     73\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet_param\u001b[39m.\u001b[39mlayer) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet_param\u001b[39m.\u001b[39mlayers):\n",
      "File \u001b[1;32mc:\\Users\\rs659\\Desktop\\AIPI 590\\Group Project 1\\Github Repo\\Scene-Recognition\\Notebooks\\caffemodel2pytorch.py:41\u001b[0m, in \u001b[0;36minitialize\u001b[1;34m(caffe_proto, codegen_dir, shadow_caffe)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mprint\u001b[39m(local_caffe_proto)\n\u001b[0;32m     40\u001b[0m \u001b[39mprint\u001b[39m(codegen_dir, local_caffe_proto)\n\u001b[1;32m---> 41\u001b[0m subprocess\u001b[39m.\u001b[39;49mcheck_call([\u001b[39m'\u001b[39;49m\u001b[39mprotoc\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m--proto_path\u001b[39;49m\u001b[39m'\u001b[39;49m, os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mdirname(local_caffe_proto), \u001b[39m'\u001b[39;49m\u001b[39m--python_out\u001b[39;49m\u001b[39m'\u001b[39;49m, codegen_dir, local_caffe_proto])\n\u001b[0;32m     42\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39minsert(\u001b[39m0\u001b[39m, codegen_dir)\n\u001b[0;32m     43\u001b[0m old_symdb \u001b[39m=\u001b[39m google\u001b[39m.\u001b[39mprotobuf\u001b[39m.\u001b[39msymbol_database\u001b[39m.\u001b[39m_DEFAULT\n",
      "File \u001b[1;32mc:\\Users\\rs659\\Desktop\\AIPI 590\\Group Project 1\\Github Repo\\Scene-Recognition\\venv\\Lib\\subprocess.py:408\u001b[0m, in \u001b[0;36mcheck_call\u001b[1;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_call\u001b[39m(\u001b[39m*\u001b[39mpopenargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    399\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Run command with arguments.  Wait for command to complete.  If\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[39m    the exit code was zero then return, otherwise raise\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[39m    CalledProcessError.  The CalledProcessError object will have the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[39m    check_call([\"ls\", \"-l\"])\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 408\u001b[0m     retcode \u001b[39m=\u001b[39m call(\u001b[39m*\u001b[39;49mpopenargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    409\u001b[0m     \u001b[39mif\u001b[39;00m retcode:\n\u001b[0;32m    410\u001b[0m         cmd \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39margs\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rs659\\Desktop\\AIPI 590\\Group Project 1\\Github Repo\\Scene-Recognition\\venv\\Lib\\subprocess.py:389\u001b[0m, in \u001b[0;36mcall\u001b[1;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39m*\u001b[39mpopenargs, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    382\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Run command with arguments.  Wait for command to complete or\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[39m    timeout, then return the returncode attribute.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39m    retcode = call([\"ls\", \"-l\"])\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 389\u001b[0m     \u001b[39mwith\u001b[39;00m Popen(\u001b[39m*\u001b[39;49mpopenargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mas\u001b[39;00m p:\n\u001b[0;32m    390\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    391\u001b[0m             \u001b[39mreturn\u001b[39;00m p\u001b[39m.\u001b[39mwait(timeout\u001b[39m=\u001b[39mtimeout)\n",
      "File \u001b[1;32mc:\\Users\\rs659\\Desktop\\AIPI 590\\Group Project 1\\Github Repo\\Scene-Recognition\\venv\\Lib\\subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[0;32m   1022\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_mode:\n\u001b[0;32m   1023\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr,\n\u001b[0;32m   1024\u001b[0m                     encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39merrors)\n\u001b[1;32m-> 1026\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0;32m   1027\u001b[0m                         pass_fds, cwd, env,\n\u001b[0;32m   1028\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[0;32m   1029\u001b[0m                         p2cread, p2cwrite,\n\u001b[0;32m   1030\u001b[0m                         c2pread, c2pwrite,\n\u001b[0;32m   1031\u001b[0m                         errread, errwrite,\n\u001b[0;32m   1032\u001b[0m                         restore_signals,\n\u001b[0;32m   1033\u001b[0m                         gid, gids, uid, umask,\n\u001b[0;32m   1034\u001b[0m                         start_new_session, process_group)\n\u001b[0;32m   1035\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m   1036\u001b[0m     \u001b[39m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mfilter\u001b[39m(\u001b[39mNone\u001b[39;00m, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdin, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr)):\n",
      "File \u001b[1;32mc:\\Users\\rs659\\Desktop\\AIPI 590\\Group Project 1\\Github Repo\\Scene-Recognition\\venv\\Lib\\subprocess.py:1538\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[39m# Start the process\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1538\u001b[0m     hp, ht, pid, tid \u001b[39m=\u001b[39m _winapi\u001b[39m.\u001b[39;49mCreateProcess(executable, args,\n\u001b[0;32m   1539\u001b[0m                              \u001b[39m# no special security\u001b[39;49;00m\n\u001b[0;32m   1540\u001b[0m                              \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   1541\u001b[0m                              \u001b[39mint\u001b[39;49m(\u001b[39mnot\u001b[39;49;00m close_fds),\n\u001b[0;32m   1542\u001b[0m                              creationflags,\n\u001b[0;32m   1543\u001b[0m                              env,\n\u001b[0;32m   1544\u001b[0m                              cwd,\n\u001b[0;32m   1545\u001b[0m                              startupinfo)\n\u001b[0;32m   1546\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1547\u001b[0m     \u001b[39m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1548\u001b[0m     \u001b[39m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[39m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m     \u001b[39m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1553\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1554\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1555\u001b[0m                          errread, errwrite)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import caffemodel2pytorch\n",
    "\n",
    "model = caffemodel2pytorch.Net(\n",
    "\tprototxt = 'deploy_10.prototxt',\n",
    "\tweights = 'snapshot_iter_765280.caffemodel',\n",
    "\tcaffe_proto = 'https://raw.githubusercontent.com/BVLC/caffe/master/src/caffe/proto/caffe.proto'\n",
    "\t# caffe_proto = \"C:\\\\Users\\\\rs659\\\\Desktop\\\\AIPI 590\\\\Group Project 1\\\\Github Repo\\\\Scene-Recognition\\\\Notebooks\\\\caffe.proto\"   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
